{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hmc_tomography import Distributions\n",
    "from hmc_tomography import Samplers\n",
    "from hmc_tomography import Samples\n",
    "from hmc_tomography import Visualization\n",
    "\n",
    "# Our trusty imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Starting simple, sampling the standard normal distribution\n",
    "\n",
    "First we create an instance of this distribution. This allows us to compute misfits and gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_simple_distribution = Distributions.StandardNormal1D()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a sampler and set the important parameters.\n",
    "\n",
    "**Try finding these s.t. the acceptance rate is about 0.63!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change things here ----------------------------------------------------------------------------<<<\n",
    "# Pick a sampler from the submodule, and instantiate it. Try out with RWMH too!\n",
    "sampler = Samplers.HMC()\n",
    "# Don't sample too long, don't sample too short\n",
    "proposals = 1000\n",
    "# Ideal acceptance rate is 0.63 (HMC) for Gaussians, find a stepsize that does this.\n",
    "stepsize = 0.5\n",
    "# ----------------------------------------------------------------------------------------\n",
    "\n",
    "# Start sampling\n",
    "sampler.sample(\n",
    "    \"samples_simple_distribution.h5\",\n",
    "    a_simple_distribution,\n",
    "    proposals=proposals,\n",
    "    stepsize=stepsize,\n",
    "    overwrite_existing_file=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's look at the results:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Samples(\"samples_simple_distribution.h5\") as samples:\n",
    "    print(\"Samples object content:\", samples.numpy)\n",
    "    print(f\"\\r\\nWith shape: {samples.numpy.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good, this thing contains all the samples `[0,:]` and all the misfits `[1,:]`.\n",
    "\n",
    "**Let's visualize them:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Samples(\"samples_simple_distribution.h5\") as samples:\n",
    "\n",
    "    # FILL IN HERE ------------------------------------------------------------------------------<<<\n",
    "    parameter_0 = None  # how can you extract the values from samples?\n",
    "    misfit = None  # How can you extract the misfits from samples?\n",
    "    # ---------------------------------------------------------------------\n",
    "\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(parameter_0, color=\"k\", label=\"parameter 0\")\n",
    "    plt.xlabel(\"sample index\")\n",
    "    plt.ylabel(\"parameter value\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.semilogy(misfit, \"r\")\n",
    "    plt.xlabel(\"sample index\")\n",
    "    plt.ylabel(\"misfit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A lot of the time, histograms are much easier to read:**\n",
    "\n",
    "Try changing the bins value to see the effect on the histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.hist(parameter_0, label=\"parameter 0\", color=\"k\", bins=25, density=True)\n",
    "plt.xlabel(\"parameter value\")\n",
    "plt.ylabel(\"density\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.hist(misfit, color=\"r\", bins=25)  # Change this guy! bins=5, bins=100! ----------------------<<<\n",
    "plt.xlabel(\"misfit\")\n",
    "plt.ylabel(\"count\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's compare the sampling result to the exact answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_0 = np.linspace(-3, 3, 1000)\n",
    "misfit_exact = np.exp(-0.5 * (par_0 ** 2)) / (np.pi * 2) ** 0.5\n",
    "plt.plot(par_0, misfit_exact, label=\"True probability\")\n",
    "plt.xlabel(\"parameter value\")\n",
    "plt.hist(\n",
    "    parameter_0, label=\"parameter 0 sampling results\", color=\"k\", bins=25, density=True\n",
    ")\n",
    "plt.xlabel(\"parameter value\")\n",
    "plt.ylabel(\"density\")\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All this plotting stuff becomes boring quick, so here's an easy function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Samples(\"samples_simple_distribution.h5\") as samples:\n",
    "    Visualization.marginal(samples, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Small upgrade, sampling a non-linear distribution\n",
    "\n",
    "Here we sample a 1-dimensional Laplace distribution, which I have artificially bounded at -0.25:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.array([[1.5]])\n",
    "disperion = np.array([[0.75]])\n",
    "lower_bound = np.array([[-0.25]])\n",
    "\n",
    "a_more_complicated_distribution = Distributions.Laplace(\n",
    "    mean, disperion, lower_bounds=lower_bound\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Changing the stepsize becomes really boring. Below, you can try enabling _autotuning_. WOW!** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = Samplers.HMC()\n",
    "\n",
    "proposals = 25000\n",
    "stepsize = 0.5e2\n",
    "\n",
    "\n",
    "sampler.sample(\n",
    "    \"a_more_complicated_distribution.h5\",\n",
    "    a_more_complicated_distribution,\n",
    "    proposals=proposals,\n",
    "    stepsize=stepsize,\n",
    "    autotuning=False,  # Try enabling this. ----------------------------------------------------------<<<\n",
    "    overwrite_existing_file=True,\n",
    "    learning_rate=0.51,  # Only change this if you're bored.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the stepsizes is always a good idea to see if we will need any burn-in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler.plot_stepsizes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check our marginal now. \n",
    "\n",
    "**If you think the marginal is incorrent, try visualizing it while accounting for burn in. ðŸ”¥ðŸ”¥** E.g. try discarding the first 1000 samples by setting burn in equal to that number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Samples(\n",
    "    \"a_more_complicated_distribution.h5\", burn_in=0  # Check this line for burn-in ------------------<<<\n",
    ") as samples:\n",
    "    Visualization.marginal(samples, 0, bins=50)\n",
    "\n",
    "    plt.plot(samples.misfits, \"r\")\n",
    "    plt.xlabel(\"sample index\")\n",
    "    plt.ylabel(\"misfit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: The ultimate test for a statistician, BAYES RULE\n",
    "\n",
    "What if we **combine** two distributions? Should one be able to hav esuch power?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = Distributions.BayesRule(\n",
    "    [a_simple_distribution, a_more_complicated_distribution]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work pretty flawlessly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposals = 50000\n",
    "\n",
    "sampler.sample(\n",
    "    \"posterior_samples.h5\",\n",
    "    posterior,\n",
    "    proposals=proposals,\n",
    "    autotuning=True,  # Cheating, but hey\n",
    "    overwrite_existing_file=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, assess burn-in to be safe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler.plot_stepsizes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Samples(\n",
    "    \"posterior_samples.h5\",\n",
    "    burn_in=0,  # Change that number in here to something reasonable\n",
    ") as samples:\n",
    "    Visualization.marginal(samples, 0, bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4: Your _own_ inverse problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax  # This package is too cool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making your own class in this package is really easy! There is only one thing missing here still, **the forward model calculation. Write this guy yourself.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarthquakeLocation1D(Distributions._AbstractDistribution):\n",
    "    \"1D earthquake location for 2 events with unknown velocity.\"\n",
    "\n",
    "    dimensions = 3\n",
    "\n",
    "    def __init__(self, observation_1, observation_2, uncertainty):\n",
    "\n",
    "        # Storing the observed data\n",
    "        self.observation_1 = observation_1\n",
    "        self.observation_2 = observation_2\n",
    "        self.uncertainty = uncertainty\n",
    "\n",
    "        # The \"I didnt't like calculus\" approach, choice #1 in Silicon Valley\n",
    "        self.gradient = jax.jit(jax.grad(self.misfit))\n",
    "\n",
    "    def misfit(self, model_vector):\n",
    "        \"\"\"The misfit function. Typically contains a forward model and a measure that\n",
    "        compares synthetics with observations, such as the L2 misfit.\"\"\"\n",
    "\n",
    "        # Deconstruct the vector\n",
    "        medium_velocity = model_vector[0, 0]\n",
    "        distance_event_1 = model_vector[1, 0]\n",
    "        distance_event_2 = model_vector[2, 0]\n",
    "\n",
    "        predicted_arrival_time_event_1 = None  # You can do this, come on!\n",
    "        predicted_arrival_time_event_2 = None  # ------------------------------------------------------<<<\n",
    "\n",
    "        data_residual_1 = self.observation_1 - predicted_arrival_time_event_1\n",
    "        data_residual_2 = self.observation_2 - predicted_arrival_time_event_2\n",
    "\n",
    "        l2_misfit = (\n",
    "            0.5 * (data_residual_1 ** 2 + data_residual_2 ** 2) / (uncertainty ** 2)\n",
    "        )\n",
    "\n",
    "        return l2_misfit\n",
    "\n",
    "    def gradient(model_vector):\n",
    "        \"I didn't do a PhD to derive analytical gradients.\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's 'create' some true data! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True parameters to find in the inversion:\n",
    "# medium_velocity = 2.75 km/s\n",
    "# distance_event_1 = 55.825 km\n",
    "# distance_event_2 = 10.175 km\n",
    "\n",
    "# Observed arrival times (uncertainty = 1.0 sec)\n",
    "arrival_time_1 = 20.3  #        = 55.825/2.75\n",
    "arrival_time_2 = 3.7  #         = 10.175/2.75\n",
    "uncertainty = 1.0\n",
    "\n",
    "# And wrapping everything up in a likelihood function.\n",
    "eq_likelihood = EarthquakeLocation1D(arrival_time_1, arrival_time_2, uncertainty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make our starting model be **something really really bad**, just to prove how good my code is (spoiler, it isn't that great)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_starting = np.array([[4.0], [10.0], [10.0]])\n",
    "\n",
    "# Initial misfit and gradient, just to check all the bits and pieces are moving.\n",
    "m = eq_likelihood.misfit(m_starting)\n",
    "g = eq_likelihood.gradient(m_starting)\n",
    "\n",
    "print(m)\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A finite difference test to make sure our fancy 3rd party software _JAX_ does what it should do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_finite_differences(f, x):\n",
    "    eps = 1e-10\n",
    "    g = np.empty_like(x)\n",
    "    for i, ix in enumerate(x):\n",
    "        x_acc = x.copy()\n",
    "        x_acc[i] += eps\n",
    "        g[i] = (f(x_acc) - f(x)) / eps\n",
    "    return g\n",
    "\n",
    "\n",
    "first_finite_differences(eq_likelihood.misfit, m_starting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's pretty cool!\n",
    "\n",
    "Now, on to sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = Samplers.RWMH()\n",
    "\n",
    "sampler.sample(\n",
    "    \"samples_eq.h5\",\n",
    "    eq_likelihood,\n",
    "    stepsize=1e-1,\n",
    "    proposals=100000,\n",
    "    online_thinning=10,\n",
    "    initial_model=m_starting,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Samples(\"samples_eq.h5\", burn_in=0) as samples:\n",
    "    Visualization.marginal_grid(samples, [0, 1, 2], bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a pretty bad result, maybe we should constrain ourselves a bit more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vector for conveniece\n",
    "ones = np.ones((1, 1))\n",
    "\n",
    "# Prior normal distribution on velocity\n",
    "prior_velocity = Distributions.Normal(ones * 2.65, ones * (0.1 ** 2))\n",
    "\n",
    "# Extremely wide uniform on distance\n",
    "prior_distance_1 = Distributions.Uniform(lower_bounds=ones * 0, upper_bounds=ones * 100)\n",
    "prior_distance_2 = prior_distance_1\n",
    "\n",
    "prior = Distributions.CompositeDistribution(\n",
    "    [prior_velocity, prior_distance_1, prior_distance_2]\n",
    ")\n",
    "\n",
    "print(prior.dimensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining our data and prior to make a posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eq_posterior = Distributions.BayesRule([prior, eq_likelihood])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = Samplers.HMC()\n",
    "\n",
    "sampler.sample(\n",
    "    \"samples_eq_posterior.h5\",\n",
    "    eq_posterior,\n",
    "    stepsize=1e-1,\n",
    "    proposals=1000,\n",
    "    online_thinning=1,\n",
    "    initial_model=m_starting,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Samples(\"samples_eq_posterior.h5\", burn_in=0) as samples:\n",
    "    Visualization.marginal_grid(samples, [0, 1, 2], bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using an estimate of the posterior covariance, we can dramaticallyt speed up sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = Samplers.RWMH()\n",
    "\n",
    "stepsize = np.array([[0.1], [3.0], [2.5]])\n",
    "\n",
    "sampler.sample(\n",
    "    \"samples_eq_posterior_rwmh.h5\",\n",
    "    eq_posterior,\n",
    "    stepsize=stepsize,\n",
    "    proposals=100000,\n",
    "    online_thinning=10,\n",
    "    initial_model=m_starting,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Samples(\"samples_eq_posterior_rwmh.h5\", burn_in=0) as samples:\n",
    "    Visualization.marginal_grid(samples, [0, 1, 2], bins=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Great success!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
