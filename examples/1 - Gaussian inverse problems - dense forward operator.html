

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Tutorial 1 - Gaussian inverse problems - dense forward operator &mdash; HMC Tom 0.2.2-alpha-5-21-gb74f3b8 documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/theme_overrides.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Tutorial 2 - Gaussian inverse problems - sparse forward operator" href="2%20-%20Gaussian%20inverse%20problems%20-%20sparse%20forward%20operator.html" />
    <link rel="prev" title="Tutorial 0.2 - Tuning Hamiltonian Monte Carlo" href="0.2%20-%20Tuning%20Hamiltonian%20Monte%20Carlo.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> HMC Tom
          

          
            
            <img src="../_static/hmctom.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                0.2.2-alpha-5-21-gb74f3b8
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../index.html">HMC Tomography</a></li>
<li class="toctree-l1"><a class="reference internal" href="../setup.html">Installation</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../examples.html">Examples and tutorials</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="0.1%20-%20Getting%20started.html">Tutorial 0.1 - Getting started</a></li>
<li class="toctree-l2"><a class="reference internal" href="0.2%20-%20Tuning%20Hamiltonian%20Monte%20Carlo.html">Tutorial 0.2 - Tuning Hamiltonian Monte Carlo</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Tutorial 1 - Gaussian inverse problems - dense forward operator</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Case-1:-diagonal-covariance-matrix---no-prior">Case 1: diagonal covariance matrix - no prior</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Case-2:-dense-covariance-matrix---no-prior">Case 2: dense covariance matrix - no prior</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Case-3:-dense-covariance-matrix---Gaussian-prior">Case 3: dense covariance matrix - Gaussian prior</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Case-4---A-non-Gaussian-prior">Case 4 - A non-Gaussian prior</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2%20-%20Gaussian%20inverse%20problems%20-%20sparse%20forward%20operator.html">Tutorial 2 - Gaussian inverse problems - sparse forward operator</a></li>
<li class="toctree-l2"><a class="reference internal" href="3%20-%20Separate%20priors%20per%20dimension.html">Tutorial 3 - Separate priors per dimension</a></li>
<li class="toctree-l2"><a class="reference internal" href="4%20-%20Creating%20your%20own%20inverse%20problem.html">Tutorial 4 - Creating your own inverse problem</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../api/index.html">API reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../py-modindex.html">Module index</a></li>
<li class="toctree-l1"><a class="reference internal" href="../genindex.html">Alphabetic index</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">HMC Tom</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../examples.html">Examples and tutorials</a> &raquo;</li>
        
      <li>Tutorial 1 - Gaussian inverse problems - dense forward operator</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/examples/1 - Gaussian inverse problems - dense forward operator.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt a.copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">hmc_tomography</span>
<span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">numpy</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">suppress</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="Tutorial-1---Gaussian-inverse-problems---dense-forward-operator">
<h1>Tutorial 1 - Gaussian inverse problems - dense forward operator<a class="headerlink" href="#Tutorial-1---Gaussian-inverse-problems---dense-forward-operator" title="Permalink to this headline">¶</a></h1>
<p><strong>A verification of HMC on analytically solvable forward models</strong></p>
<p>In this notebook, we’ll look at sampling of inverse problems to which we have exact solutions, both the verify the validity of MCMC methods (HMC) and to illustrate how one could extend these simple problems with interesting (non-Gaussian) prior information.</p>
<div class="admonition note">
<p>Note: Throughout this notebook, we’ll sample various distributions. Tuning settings are updated accordingly, but not discussed. Keep this in mind when playing around with the notebook; you might need to update parameters, primarily step lengths.</p>
</div>
<p>We will first define the model and data space size. Model parameters and data in linear forward problems are related with the following equation:</p>
<div class="math notranslate nohighlight">
\[\mathbf{d} = G\mathbf{m}\]</div>
<p>Here, <span class="math notranslate nohighlight">\(\mathbf{d}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{m}\)</span> are column vectors of resp. shape <span class="math notranslate nohighlight">\([d \times 1]\)</span> and <span class="math notranslate nohighlight">\([m \times 1]\)</span>, and G has size <span class="math notranslate nohighlight">\([m \times d]\)</span>. Let’s start of with choosing these sizes for our illustration (feel free to experiment with these):</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">dimensions</span> <span class="o">=</span> <span class="mi">6</span>
<span class="n">dpoints</span> <span class="o">=</span> <span class="mi">12</span>
</pre></div>
</div>
</div>
<p>Next, we randomly generate a dense forward model operator. We are not interested in the exact values as of yet, but it is important to realise that by randomly generating values, we are not requiring G to be of full rank, i.e. (pseudo-)invertible.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="mi">42</span> <span class="o">+</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">()))</span>
<span class="n">G</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">dpoints</span><span class="p">,</span> <span class="n">dimensions</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Next we generate a ‘true’ model. Note that this model is <em>extremely</em> sparse, only two entries are non-zero.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">m_true</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">dimensions</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">m_true</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mf">2.0</span>
<span class="n">m_true</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>
</pre></div>
</div>
</div>
<p>We generate true data, but we don’t do this deterministically. We directly choose to generate data with a specific noise model. This noise model is assumed known for this notebook.</p>
<p>The observed data is generated using the following relation:</p>
<div class="math notranslate nohighlight">
\[\mathbf{d}_\text{obs} = G\mathbf{m}_\text{true} + \mathbf{\sigma}\]</div>
<p>Note that the added noise is also vector. We typically assume it’s normally distributed (using a multivariate Gaussian):</p>
<div class="math notranslate nohighlight">
\[\mathbf{\sigma} \sim \mathcal{N} (\mathbf{0} , \Sigma)\]</div>
<p>which makes inference in general much easier. Then the observed data constitute the noise-less observations together with a realization of this distribution.</p>
<p>The description of the noise may depend strongly on which datapoint we look at. There could be correlations and strong differences in standard deviation for each datum <span class="math notranslate nohighlight">\(d_i\)</span>, as captured in the covariance matrix <span class="math notranslate nohighlight">\(\Sigma\)</span>. We will first start with a diagonal (non-correlated) data covariance matrix. <strong>NOTE</strong>: we define data standard deviation in the next cell. To obtain <span class="math notranslate nohighlight">\(\Sigma\)</span>, we need to compute the square of the standard deviation matrix.</p>
<div class="section" id="Case-1:-diagonal-covariance-matrix---no-prior">
<h2>Case 1: diagonal covariance matrix - no prior<a class="headerlink" href="#Case-1:-diagonal-covariance-matrix---no-prior" title="Permalink to this headline">¶</a></h2>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># An array describing the noise for every datum separately</span>
<span class="n">data_std</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dpoints</span> <span class="o">*</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,])[:,</span> <span class="kc">None</span><span class="p">]</span>

<span class="c1"># Manually change one datum&#39;s noise</span>
<span class="n">data_std</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mf">10.0</span>
<span class="n">data_std</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">=</span> <span class="mf">10.0</span>

<span class="c1"># Generate the data</span>
<span class="n">d_obs</span> <span class="o">=</span> <span class="n">G</span> <span class="o">@</span> <span class="n">m_true</span> <span class="o">+</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">G</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">data_std</span>
</pre></div>
</div>
</div>
<p>Now we have everything we need to define our (non-regularized / prior-less) inference problem using a provided class. This object analyses the supplied objects and determines the appropriate methods (i.e. sparse vs dense <span class="math notranslate nohighlight">\(G\)</span>, dense vs diagonal <span class="math notranslate nohighlight">\(\Sigma\)</span>, etc.).</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">likelihood</span> <span class="o">=</span> <span class="n">hmc_tomography</span><span class="o">.</span><span class="n">Distributions</span><span class="o">.</span><span class="n">LinearMatrix</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">d_obs</span><span class="p">,</span> <span class="n">data_std</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>To validate sampling, we first compute the analytical posterior Gaussian. Check Tarantola (2005), page 36, example 1.38 for the relevant expressions. Note that a ‘prior’ is still absent, so formulas have been modified accordingly.</p>
<p>Because the LinearMatrix class also allows for scalar data covariance, we added (but commented out) expressions for this case</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Analytical results</span>

<span class="c1"># Covariance matrix</span>
<span class="c1"># COV = numpy.linalg.inv(G.T @ G / (data_std ** 2)) # Scalar data covariance</span>
<span class="n">inv_data_cov</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">data_std</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">COV</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">G</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">inv_data_cov</span> <span class="o">@</span> <span class="n">G</span><span class="p">)</span>  <span class="c1"># Vector data covariance</span>

<span class="c1"># Standard deviations</span>
<span class="n">SIGMA</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">COV</span><span class="p">)</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">)</span>

<span class="c1"># Correlation structure</span>
<span class="n">COR</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">SIGMA</span><span class="p">)</span> <span class="o">@</span> <span class="n">COV</span> <span class="o">@</span> <span class="n">numpy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">SIGMA</span><span class="p">)</span>

<span class="c1"># Means</span>
<span class="n">MEAN</span> <span class="o">=</span> <span class="n">COV</span> <span class="o">@</span> <span class="n">G</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">inv_data_cov</span> <span class="o">@</span> <span class="n">d_obs</span>
</pre></div>
</div>
</div>
<p>If you encounter the warning: <code class="docutils literal notranslate"><span class="pre">invalid</span> <span class="pre">value</span> <span class="pre">encountered</span> <span class="pre">in</span> <span class="pre">sqrt</span></code> you most likely have more model parameters than observations. In this case, your likelihood covariance matrix can not be inverted (because it is guaranteed not full rank) and we probably should add a prior term that is well behaved.</p>
<p>What this means in a probabilistic interpretation is that the resulting distribution has infinite support and is non-normalizable, and we should use Bayes’ rule to counter this.</p>
<p><strong>Let’s start sampling!</strong></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">hmc_tomography</span><span class="o">.</span><span class="n">Samplers</span><span class="o">.</span><span class="n">HMC</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span>
    <span class="n">likelihood</span><span class="p">,</span>
    <span class="s2">&quot;bin_samples/tutorial_1_diagonal_covariance.h5&quot;</span><span class="p">,</span>
    <span class="n">mass_matrix</span><span class="o">=</span><span class="n">hmc_tomography</span><span class="o">.</span><span class="n">MassMatrices</span><span class="o">.</span><span class="n">Unit</span><span class="p">(</span><span class="n">likelihood</span><span class="o">.</span><span class="n">dimensions</span><span class="p">),</span>
    <span class="n">proposals</span><span class="o">=</span><span class="mi">30000</span><span class="p">,</span>
    <span class="n">integration_steps</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
    <span class="n">time_step</span><span class="o">=</span><span class="mf">0.07</span><span class="p">,</span>
    <span class="n">online_thinning</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">overwrite_samples</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">initial_model</span><span class="o">=</span><span class="n">m_true</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
sys:1: Warning:
Silently overwriting samples file (bin_samples/tutorial_1_diagonal_covariance.h5) if it exists.
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "ab45fd4b34fb4505bcd1370a3b0fb301", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>

</pre></div></div>
</div>
<p>Let’s now compute the same quantities using the samples. The <code class="docutils literal notranslate"><span class="pre">with</span></code> construct is essential, as it closes the sample file after we exit the indent. This ensures that we don’t leave open the HDF5 file. <strong>Additionally, we’ll visualize the 1d and 2d marginals of the posterior.</strong></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="n">hmc_tomography</span><span class="o">.</span><span class="n">Post</span><span class="o">.</span><span class="n">Samples</span><span class="p">(</span>
    <span class="s2">&quot;bin_samples/tutorial_1_diagonal_covariance.h5&quot;</span><span class="p">,</span> <span class="n">burn_in</span><span class="o">=</span><span class="mi">1000</span>
<span class="p">)</span> <span class="k">as</span> <span class="n">samples</span><span class="p">:</span>

    <span class="n">hmc_tomography</span><span class="o">.</span><span class="n">Post</span><span class="o">.</span><span class="n">Visualization</span><span class="o">.</span><span class="n">marginal_grid</span><span class="p">(</span>
        <span class="n">samples</span><span class="p">,</span>
        <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>  <span class="c1"># Mind the ZERO INDEXED dimensions.</span>
        <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">12</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="n">sampling_MEAN</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">samples</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">sampling_COV</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">samples</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:])</span>

    <span class="n">sampling_SIGMA</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">sampling_COV</span><span class="p">)</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">)</span>

    <span class="n">sampling_COR</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">numpy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">sampling_SIGMA</span><span class="p">)</span>
        <span class="o">@</span> <span class="n">sampling_COV</span>
        <span class="o">@</span> <span class="n">numpy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">sampling_SIGMA</span><span class="p">)</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/examples_1_-_Gaussian_inverse_problems_-_dense_forward_operator_21_0.png" src="../_images/examples_1_-_Gaussian_inverse_problems_-_dense_forward_operator_21_0.png" />
</div>
</div>
<p>Now we compare the posterior covariance and means of the two methods. We hope these two results are the same (as they theoretically should be), otherwise we probably made an error in the code.</p>
<p>We’ll visualize two different things: 1. The means and variances of the sampling and analytical posteriors; 2. The full covariance matrices of the sampling and analytical posteriors.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">m_true</span><span class="o">.</span><span class="n">size</span><span class="p">),</span> <span class="n">m_true</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;True value&quot;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;v&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">)</span>


<span class="n">ax</span><span class="o">.</span><span class="n">errorbar</span><span class="p">(</span>
    <span class="n">numpy</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">sampling_MEAN</span><span class="o">.</span><span class="n">size</span><span class="p">),</span>
    <span class="n">sampling_MEAN</span><span class="p">,</span>
    <span class="n">capsize</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
    <span class="n">yerr</span><span class="o">=</span><span class="n">numpy</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">sampling_SIGMA</span><span class="p">),</span>
    <span class="n">fmt</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Sampling&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">errorbar</span><span class="p">(</span>
    <span class="n">numpy</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">MEAN</span><span class="o">.</span><span class="n">size</span><span class="p">),</span>
    <span class="n">MEAN</span><span class="p">,</span>
    <span class="n">capsize</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
    <span class="n">yerr</span><span class="o">=</span><span class="n">numpy</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">SIGMA</span><span class="p">),</span>
    <span class="n">fmt</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Analytical&quot;</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">2.25</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Parameter estimate&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Parameter index&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Parameter estimates</span><span class="se">\n</span><span class="s2">means: markers, sigma: lines&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span>
    <span class="s2">&quot;BE AWARE, I print only up to 2 digits. &quot;</span>
    <span class="s2">&quot;The sampling covariance will appear less &quot;</span>
    <span class="s2">&quot;accurate when printing more digits.&quot;</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Analytical covariance matrix:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">COV</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sampling covariance matrix:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sampling_COV</span><span class="p">)</span>

<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Difference:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sampling_COV</span> <span class="o">-</span> <span class="n">COV</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/examples_1_-_Gaussian_inverse_problems_-_dense_forward_operator_23_0.png" src="../_images/examples_1_-_Gaussian_inverse_problems_-_dense_forward_operator_23_0.png" />
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
BE AWARE, I print only up to 2 digits. The sampling covariance will appear less accurate when printing more digits.

Analytical covariance matrix:
[[ 0.32  0.12  0.07 -0.34 -0.02 -0.17]
 [ 0.12  0.15  0.04 -0.2  -0.04 -0.04]
 [ 0.07  0.04  0.05 -0.09 -0.02 -0.04]
 [-0.34 -0.2  -0.09  0.46  0.02  0.14]
 [-0.02 -0.04 -0.02  0.02  0.08 -0.01]
 [-0.17 -0.04 -0.04  0.14 -0.01  0.15]]

Sampling covariance matrix:
[[ 0.32  0.12  0.07 -0.34 -0.02 -0.17]
 [ 0.12  0.16  0.04 -0.21 -0.04 -0.04]
 [ 0.07  0.04  0.05 -0.1  -0.02 -0.04]
 [-0.34 -0.21 -0.1   0.46  0.02  0.15]
 [-0.02 -0.04 -0.02  0.02  0.08 -0.01]
 [-0.17 -0.04 -0.04  0.15 -0.01  0.15]]

Difference:
[[ 0.  0.  0. -0. -0. -0.]
 [ 0.  0.  0. -0. -0.  0.]
 [ 0.  0.  0. -0. -0. -0.]
 [-0. -0. -0.  0.  0.  0.]
 [-0. -0. -0.  0.  0. -0.]
 [-0.  0. -0.  0. -0.  0.]]
</pre></div></div>
</div>
<p>Well, everything seems to be in order! Let’s extend the theory a little bit with a dense covariance matrix.</p>
</div>
<div class="section" id="Case-2:-dense-covariance-matrix---no-prior">
<h2>Case 2: dense covariance matrix - no prior<a class="headerlink" href="#Case-2:-dense-covariance-matrix---no-prior" title="Permalink to this headline">¶</a></h2>
<p>In this case, we want to encode correlations between the errors in the observed data. This may be done with normally distributed noise by adding non-diagonal entries in the covariance matrix <span class="math notranslate nohighlight">\(\Sigma\)</span>. To make life easier, we can decompose the covariance matrix (using Eigendecomposition) into variances (diagonal matrix with variances on the diagonal, <span class="math notranslate nohighlight">\(\text{diag}(\mathbf{\sigma})\)</span>) and the correlation matrix (<span class="math notranslate nohighlight">\(R\)</span>):</p>
<div class="math notranslate nohighlight">
\[\Sigma = \text{diag}(\mathbf{\sigma}) \:\: R \:\:
\text{diag} (\mathbf{\sigma})\]</div>
<p>Correlations always lie between -1 and 1, and are 1 for correlation between one parameter and itself. Both the covariance matrix and the correlation matrix have to be positive definite, otherwise the multivariate Gaussian is non-normalizable (if positive-semidefinite) or not even a proper probability (if anything else).</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># An array describing the noise for every datum separately</span>
<span class="n">data_std_dense</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dpoints</span> <span class="o">*</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,])</span>

<span class="c1"># Manually change one datum&#39;s noise (same as previous example)</span>
<span class="n">data_std_dense</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">data_std_dense</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">data_std_dense</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">data_std_dense</span><span class="p">)</span>

<span class="n">cor</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">data_std_dense</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">data_std_dense</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
    <span class="k">for</span> <span class="n">diff</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">data_std_dense</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)):</span>
        <span class="n">cor</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">i</span> <span class="o">-</span> <span class="n">diff</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.7</span> <span class="o">**</span> <span class="p">(</span><span class="n">diff</span> <span class="o">/</span> <span class="mf">5.0</span> <span class="o">+</span> <span class="mf">1.0</span><span class="p">)</span>
        <span class="n">cor</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="n">diff</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.7</span> <span class="o">**</span> <span class="p">(</span><span class="n">diff</span> <span class="o">/</span> <span class="mf">5.0</span> <span class="o">+</span> <span class="mf">1.0</span><span class="p">)</span>

<span class="n">data_covariance_dense</span> <span class="o">=</span> <span class="n">data_std_dense</span> <span class="o">@</span> <span class="n">cor</span> <span class="o">@</span> <span class="n">data_std_dense</span>

<span class="c1"># Positive definiteness check</span>
<span class="k">if</span> <span class="n">numpy</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigvals</span><span class="p">(</span><span class="n">data_covariance_dense</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Matrix is positive definite; proper covariance matrix.&quot;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Not a valid covariance matrix; not positive definite.&quot;</span><span class="p">)</span>

<span class="c1"># Upper cholesky decomposition to generate RNG</span>
<span class="n">CHOL</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">data_covariance_dense</span><span class="p">)</span>

<span class="c1"># Generate the data with the updated noise model</span>
<span class="n">dense_d_obs</span> <span class="o">=</span> <span class="n">G</span> <span class="o">@</span> <span class="n">m_true</span> <span class="o">+</span> <span class="n">CHOL</span> <span class="o">@</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">G</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Matrix is positive definite; proper covariance matrix.
</pre></div></div>
</div>
<p>The matrix we create is positive definite! Now let’s visualize <span class="math notranslate nohighlight">\(\Sigma\)</span> and <span class="math notranslate nohighlight">\(R\)</span>.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">matplotlib.ticker</span> <span class="kn">import</span> <span class="n">FormatStrFormatter</span>
<span class="kn">from</span> <span class="nn">matplotlib.ticker</span> <span class="kn">import</span> <span class="n">MaxNLocator</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">ax</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_major_locator</span><span class="p">(</span><span class="n">MaxNLocator</span><span class="p">(</span><span class="n">integer</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_major_locator</span><span class="p">(</span><span class="n">MaxNLocator</span><span class="p">(</span><span class="n">integer</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="n">im</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">data_covariance_dense</span><span class="p">)))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Data covariance matrix (logarithmic)&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;data index&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;data index&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_major_formatter</span><span class="p">(</span><span class="n">FormatStrFormatter</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%i</span><span class="s2">&quot;</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">im</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&lt;matplotlib.colorbar.Colorbar at 0x7f8ae9c250b8&gt;
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/examples_1_-_Gaussian_inverse_problems_-_dense_forward_operator_29_1.png" src="../_images/examples_1_-_Gaussian_inverse_problems_-_dense_forward_operator_29_1.png" />
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">cor</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s2">&quot;seismic&quot;</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Data correlation matrix&quot;</span><span class="p">)</span>
<span class="n">cbar</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/examples_1_-_Gaussian_inverse_problems_-_dense_forward_operator_30_0.png" src="../_images/examples_1_-_Gaussian_inverse_problems_-_dense_forward_operator_30_0.png" />
</div>
</div>
<p>These observations are highly correlated. Let’s now construct our likelihood with this new data noise model.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">likelihood</span> <span class="o">=</span> <span class="n">hmc_tomography</span><span class="o">.</span><span class="n">Distributions</span><span class="o">.</span><span class="n">LinearMatrix</span><span class="p">(</span>
    <span class="n">G</span><span class="p">,</span> <span class="n">dense_d_obs</span><span class="p">,</span> <span class="n">data_covariance_dense</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<p>And compute the analytical solution of the inverse problem.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[15]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Analytical results</span>

<span class="c1"># Covariance matrix</span>
<span class="n">inv_data_cov</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">data_covariance_dense</span><span class="p">)</span>
<span class="n">dense_COV</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">G</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">inv_data_cov</span> <span class="o">@</span> <span class="n">G</span><span class="p">)</span>  <span class="c1"># Vector data covariance</span>

<span class="c1"># Standard deviations</span>
<span class="n">dense_SIGMA</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">dense_COV</span><span class="p">)</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">)</span>

<span class="c1"># Correlation structure</span>
<span class="n">dense_COR</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">dense_SIGMA</span><span class="p">)</span> <span class="o">@</span> <span class="n">dense_COV</span> <span class="o">@</span> <span class="n">numpy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">dense_SIGMA</span><span class="p">)</span>

<span class="c1"># Means</span>
<span class="n">dense_MEAN</span> <span class="o">=</span> <span class="n">dense_COV</span> <span class="o">@</span> <span class="n">G</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">inv_data_cov</span> <span class="o">@</span> <span class="n">dense_d_obs</span>
</pre></div>
</div>
</div>
<p>And draw samples from the likelihood.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[16]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">hmc_tomography</span><span class="o">.</span><span class="n">Samplers</span><span class="o">.</span><span class="n">HMC</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span>
    <span class="n">likelihood</span><span class="p">,</span>
    <span class="s2">&quot;bin_samples/tutorial_1_dense_covariance.h5&quot;</span><span class="p">,</span>
    <span class="n">mass_matrix</span><span class="o">=</span><span class="n">hmc_tomography</span><span class="o">.</span><span class="n">MassMatrices</span><span class="o">.</span><span class="n">Unit</span><span class="p">(</span><span class="n">likelihood</span><span class="o">.</span><span class="n">dimensions</span><span class="p">),</span>
    <span class="n">proposals</span><span class="o">=</span><span class="mi">20000</span><span class="p">,</span>
    <span class="n">integration_steps</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
    <span class="n">time_step</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
    <span class="n">online_thinning</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">overwrite_samples</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">initial_model</span><span class="o">=</span><span class="n">m_true</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
sys:1: Warning:
Silently overwriting samples file (bin_samples/tutorial_1_dense_covariance.h5) if it exists.
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "57ea5e8753924339b670ccbdbc5200f6", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>

</pre></div></div>
</div>
<p>Now we visualize the marginals again. Similar to the correlation matrix of the data noise, the upper right of this plot <strong>is</strong> the correlation matrix of the likelihood.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[17]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="n">hmc_tomography</span><span class="o">.</span><span class="n">Post</span><span class="o">.</span><span class="n">Samples</span><span class="p">(</span>
    <span class="s2">&quot;bin_samples/tutorial_1_dense_covariance.h5&quot;</span><span class="p">,</span> <span class="n">burn_in</span><span class="o">=</span><span class="mi">2000</span>
<span class="p">)</span> <span class="k">as</span> <span class="n">samples</span><span class="p">:</span>

    <span class="n">hmc_tomography</span><span class="o">.</span><span class="n">Post</span><span class="o">.</span><span class="n">Visualization</span><span class="o">.</span><span class="n">marginal_grid</span><span class="p">(</span>
        <span class="n">samples</span><span class="p">,</span>
        <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
        <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">12</span><span class="p">),</span>  <span class="c1"># Mind the ZERO INDEXED dimensions.</span>
    <span class="p">)</span>

    <span class="n">sampling_dense_MEAN</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">samples</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">sampling_dense_COV</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">samples</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:])</span>

    <span class="n">sampling_dense_SIGMA</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">sampling_dense_COV</span><span class="p">)</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">)</span>

    <span class="n">sampling_dense_COR</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">numpy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">sampling_dense_SIGMA</span><span class="p">)</span>
        <span class="o">@</span> <span class="n">sampling_dense_COV</span>
        <span class="o">@</span> <span class="n">numpy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">sampling_dense_SIGMA</span><span class="p">)</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/examples_1_-_Gaussian_inverse_problems_-_dense_forward_operator_38_0.png" src="../_images/examples_1_-_Gaussian_inverse_problems_-_dense_forward_operator_38_0.png" />
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[18]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
    <span class="n">numpy</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">m_true</span><span class="o">.</span><span class="n">size</span><span class="p">),</span> <span class="n">m_true</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;True value&quot;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;v&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">errorbar</span><span class="p">(</span>
    <span class="n">numpy</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">sampling_dense_MEAN</span><span class="o">.</span><span class="n">size</span><span class="p">),</span>
    <span class="n">sampling_dense_MEAN</span><span class="p">,</span>
    <span class="n">yerr</span><span class="o">=</span><span class="n">numpy</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">sampling_dense_SIGMA</span><span class="p">),</span>
    <span class="n">fmt</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span>
    <span class="n">capsize</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Sampling&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">errorbar</span><span class="p">(</span>
    <span class="n">numpy</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">dense_MEAN</span><span class="o">.</span><span class="n">size</span><span class="p">),</span>
    <span class="n">dense_MEAN</span><span class="p">,</span>
    <span class="n">yerr</span><span class="o">=</span><span class="n">numpy</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">dense_SIGMA</span><span class="p">),</span>
    <span class="n">fmt</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">,</span>
    <span class="n">capsize</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Analytical&quot;</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">errorbar</span><span class="p">(</span>
    <span class="n">numpy</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">MEAN</span><span class="o">.</span><span class="n">size</span><span class="p">),</span>
    <span class="n">MEAN</span><span class="p">,</span>
    <span class="n">yerr</span><span class="o">=</span><span class="n">numpy</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">SIGMA</span><span class="p">),</span>
    <span class="n">fmt</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">,</span>
    <span class="n">capsize</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Diagonal (previous)&quot;</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">2.25</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Parameter estimate&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Parameter index&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Parameter estimates</span><span class="se">\n</span><span class="s2">means: markers, sigma: lines&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span>
    <span class="s2">&quot;BE AWARE, I print only up to 2 digits. &quot;</span>
    <span class="s2">&quot;The sampling covariance will appear less &quot;</span>
    <span class="s2">&quot;accurate when printing more digits.&quot;</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Analytical covariance matrix:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dense_COV</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sampling covariance matrix:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sampling_dense_COV</span><span class="p">)</span>

<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Difference:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sampling_dense_COV</span> <span class="o">-</span> <span class="n">dense_COV</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/examples_1_-_Gaussian_inverse_problems_-_dense_forward_operator_39_0.png" src="../_images/examples_1_-_Gaussian_inverse_problems_-_dense_forward_operator_39_0.png" />
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
BE AWARE, I print only up to 2 digits. The sampling covariance will appear less accurate when printing more digits.

Analytical covariance matrix:
[[ 0.12  0.05  0.01 -0.13 -0.01 -0.06]
 [ 0.05  0.05  0.01 -0.07 -0.02 -0.01]
 [ 0.01  0.01  0.02 -0.02 -0.01 -0.01]
 [-0.13 -0.07 -0.02  0.18  0.    0.06]
 [-0.01 -0.02 -0.01  0.    0.03 -0.01]
 [-0.06 -0.01 -0.01  0.06 -0.01  0.05]]

Sampling covariance matrix:
[[ 0.12  0.05  0.01 -0.13 -0.01 -0.06]
 [ 0.05  0.05  0.01 -0.07 -0.02 -0.01]
 [ 0.01  0.01  0.02 -0.02 -0.01 -0.01]
 [-0.13 -0.07 -0.02  0.18  0.    0.05]
 [-0.01 -0.02 -0.01  0.    0.03 -0.01]
 [-0.06 -0.01 -0.01  0.05 -0.01  0.05]]

Difference:
[[ 0.  0. -0. -0. -0.  0.]
 [ 0.  0.  0. -0. -0.  0.]
 [-0.  0.  0.  0. -0.  0.]
 [-0. -0.  0. -0.  0. -0.]
 [-0. -0. -0.  0.  0. -0.]
 [ 0.  0.  0. -0. -0.  0.]]
</pre></div></div>
</div>
<p>There is a few important observations to made from the output here:</p>
<ol class="arabic simple">
<li><p>The sampling variances and means are equal (to a precision proportional to the amount of samples) to the analytical solution;</p></li>
<li><p>Because the noise realization and the data <strong>covariance</strong> are different, the means and variances are not equal to the previous case, even though the noise <strong>variance</strong> for every single datum is equal;</p></li>
<li><p>The total variance (sum of variances <em>or</em> product of variances) seems to be lower in the dense covariance matrix case.</p></li>
</ol>
<p>Especially point 3 was surprising to me. If there are trade-offs between datums, shouldn’t we constrain the model parameters worse? It turns out, the opposite happens. Let’s first look at the data covariances matrices to argument this.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[19]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">data_covariance_dense</span>
<span class="n">data_covariance_diagonal</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">data_std</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Data space measure of dispersion of the noise model:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;Ellipsoid volume diagonal (uncorrelated noise): </span><span class="se">\t</span><span class="si">{</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">det</span><span class="p">(</span><span class="n">data_covariance_diagonal</span><span class="p">))</span><span class="si">:</span><span class="s2">.2e</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;Ellipsoid volume dense (correlated noise): </span><span class="se">\t\t</span><span class="si">{</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">det</span><span class="p">(</span><span class="n">data_covariance_dense</span><span class="p">))</span><span class="si">:</span><span class="s2">.2e</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Data space measure of dispersion of the noise model:
Ellipsoid volume diagonal (uncorrelated noise):         1.05e-10
Ellipsoid volume dense (correlated noise):              9.06e-15
</pre></div></div>
</div>
<p>Maybe it may make intuitive sense now: The possible values the data have before the noise is added is proportional to this determinant, or volume, of the multivariate Gaussian. In the case of the correlated noise, the possible true data is a much more constrained volume.</p>
<p>Verifying these volumes on the total variability of the posteriors in model space, we see the same thing:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[20]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model space measure of dispersion of the posterior:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Ellipsoid volume uncorrelated noise: </span><span class="se">\t</span><span class="si">{</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">det</span><span class="p">(</span><span class="n">COV</span><span class="p">))</span><span class="si">:</span><span class="s2">.2e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Ellipsoid volume correlated noise: </span><span class="se">\t</span><span class="si">{</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">det</span><span class="p">(</span><span class="n">dense_COV</span><span class="p">))</span><span class="si">:</span><span class="s2">.2e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Model space measure of dispersion of the posterior:
Ellipsoid volume uncorrelated noise:    3.23e-08
Ellipsoid volume correlated noise:      2.48e-10
</pre></div></div>
</div>
</div>
<div class="section" id="Case-3:-dense-covariance-matrix---Gaussian-prior">
<h2>Case 3: dense covariance matrix - Gaussian prior<a class="headerlink" href="#Case-3:-dense-covariance-matrix---Gaussian-prior" title="Permalink to this headline">¶</a></h2>
<p>As a final validity test, we add a Gaussian prior. In this case study, the forward operator already allows us to find all medium parameters; the inverse problem is overdetermined. When this is not the case, we actually <strong>require</strong> a prior that makes the inverse well-behaved.</p>
<p>One can again find an analytical expression of the posterior in this case in Tarantola’s 2005 book, page 36. We can recycle the data covariance matrix, observed data, and likelihood function from the previous case, because the noise model did not change:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[21]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Recycled variables</span>
<span class="n">data_covariance_dense</span>
<span class="n">dense_d_obs</span>
<span class="n">likelihood</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[21]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&lt;hmc_tomography.Distributions.LinearMatrix.LinearMatrix at 0x7f8aea925908&gt;
</pre></div></div>
</div>
<p>Now we can create a prior centered around zero with a given variance.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[22]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">prior_means</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">likelihood</span><span class="o">.</span><span class="n">dimensions</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">prior_variances</span> <span class="o">=</span> <span class="mf">0.025</span> <span class="o">*</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">likelihood</span><span class="o">.</span><span class="n">dimensions</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">prior</span> <span class="o">=</span> <span class="n">hmc_tomography</span><span class="o">.</span><span class="n">Distributions</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">prior_means</span><span class="p">,</span> <span class="n">prior_variances</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
/home/larsgebraad/Documents/Hamiltonian Monte Carlo/hmc-tomography/hmc_tomography/Distributions/base.py:366: Warning: Seems that you only passed a vector as the covariance matrix. It will be used as the covariance diagonal.
  Warning,
</pre></div></div>
</div>
<p>Now, applying Bayes’ rule without normalization is extremely easy (note that this works on more than 2 distributions simultaneously too):</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[23]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">posterior</span> <span class="o">=</span> <span class="n">hmc_tomography</span><span class="o">.</span><span class="n">Distributions</span><span class="o">.</span><span class="n">BayesRule</span><span class="p">([</span><span class="n">prior</span><span class="p">,</span> <span class="n">likelihood</span><span class="p">])</span>
</pre></div>
</div>
</div>
<p>And we can compute the analytical solution again for this case.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[24]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Analytical results</span>

<span class="c1"># Covariance matrix</span>
<span class="n">inv_prior_cov</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">prior_variances</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]))</span>
<span class="n">inv_data_cov</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">data_covariance_dense</span><span class="p">)</span>
<span class="n">dense_COV_prior</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span>
    <span class="n">G</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">inv_data_cov</span> <span class="o">@</span> <span class="n">G</span> <span class="o">+</span> <span class="n">inv_prior_cov</span>
<span class="p">)</span>  <span class="c1"># Vector data covariance</span>

<span class="c1"># Standard deviations</span>
<span class="n">dense_SIGMA_prior</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">dense_COV_prior</span><span class="p">)</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">)</span>

<span class="c1"># Correlation structure</span>
<span class="n">dense_COR_prior</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">numpy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">dense_SIGMA</span><span class="p">)</span> <span class="o">@</span> <span class="n">dense_COV</span> <span class="o">@</span> <span class="n">numpy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">dense_SIGMA</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># Means</span>
<span class="n">dense_MEAN_prior</span> <span class="o">=</span> <span class="n">dense_COV_prior</span> <span class="o">@</span> <span class="p">(</span>
    <span class="n">G</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">inv_data_cov</span> <span class="o">@</span> <span class="n">dense_d_obs</span> <span class="o">+</span> <span class="n">inv_prior_cov</span> <span class="o">@</span> <span class="n">prior_means</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<p>Sampling the distribution …</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[25]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">hmc_tomography</span><span class="o">.</span><span class="n">Samplers</span><span class="o">.</span><span class="n">HMC</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span>
    <span class="n">posterior</span><span class="p">,</span>
    <span class="s2">&quot;bin_samples/tutorial_1_dense_covariance_with_gaussian_prior.h5&quot;</span><span class="p">,</span>
    <span class="n">mass_matrix</span><span class="o">=</span><span class="n">hmc_tomography</span><span class="o">.</span><span class="n">MassMatrices</span><span class="o">.</span><span class="n">Unit</span><span class="p">(</span><span class="n">likelihood</span><span class="o">.</span><span class="n">dimensions</span><span class="p">),</span>
    <span class="n">proposals</span><span class="o">=</span><span class="mi">80000</span><span class="p">,</span>
    <span class="n">time_step</span><span class="o">=</span><span class="mf">0.075</span><span class="p">,</span>
    <span class="n">overwrite_samples</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">online_thinning</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">initial_model</span><span class="o">=</span><span class="n">m_true</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
sys:1: Warning:
Silently overwriting samples file (bin_samples/tutorial_1_dense_covariance_with_gaussian_prior.h5) if it exists.
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "ac9f11b7672c489791ff375046b6fbb8", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>

</pre></div></div>
</div>
<p>Visualizing the marginals and computing the multivariate normal attributes from the samples.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[26]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="n">hmc_tomography</span><span class="o">.</span><span class="n">Post</span><span class="o">.</span><span class="n">Samples</span><span class="p">(</span>
    <span class="s2">&quot;bin_samples/tutorial_1_dense_covariance_with_gaussian_prior.h5&quot;</span><span class="p">,</span> <span class="n">burn_in</span><span class="o">=</span><span class="mi">1000</span>
<span class="p">)</span> <span class="k">as</span> <span class="n">samples_posterior</span><span class="p">:</span>

    <span class="n">hmc_tomography</span><span class="o">.</span><span class="n">Post</span><span class="o">.</span><span class="n">Visualization</span><span class="o">.</span><span class="n">marginal_grid</span><span class="p">(</span>
        <span class="n">samples_posterior</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,],</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">12</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="n">sampling_dense_MEAN_prior</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">samples_posterior</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">sampling_dense_COV_prior</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">samples_posterior</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:])</span>

    <span class="n">sampling_dense_SIGMA_prior</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">sampling_dense_COV_prior</span><span class="p">)</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">)</span>

    <span class="n">sampling_dense_COR_prior</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">numpy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">sampling_dense_SIGMA_prior</span><span class="p">)</span>
        <span class="o">@</span> <span class="n">sampling_dense_COV_prior</span>
        <span class="o">@</span> <span class="n">numpy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">sampling_dense_SIGMA_prior</span><span class="p">)</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/examples_1_-_Gaussian_inverse_problems_-_dense_forward_operator_56_0.png" src="../_images/examples_1_-_Gaussian_inverse_problems_-_dense_forward_operator_56_0.png" />
</div>
</div>
<p>Now we compare three things again:</p>
<ol class="arabic simple">
<li><p>The analytical result</p></li>
<li><p>The sampling result</p></li>
<li><p>The result of the non-regularized inversion</p></li>
</ol>
<p>We again hope that 1 and 2 are the same. Additionally, we expect the addition of a prior to reduce the variability in the posterior, while ‘pulling’ model parameters to it’s mean. Let’s see what happens:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[27]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
    <span class="n">numpy</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">m_true</span><span class="o">.</span><span class="n">size</span><span class="p">),</span> <span class="n">m_true</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;True value&quot;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;v&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">errorbar</span><span class="p">(</span>
    <span class="n">numpy</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">sampling_dense_MEAN_prior</span><span class="o">.</span><span class="n">size</span><span class="p">),</span>
    <span class="n">sampling_dense_MEAN_prior</span><span class="p">,</span>
    <span class="n">yerr</span><span class="o">=</span><span class="n">numpy</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">sampling_dense_SIGMA_prior</span><span class="p">),</span>
    <span class="n">fmt</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span>
    <span class="n">capsize</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Sampling&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">errorbar</span><span class="p">(</span>
    <span class="n">numpy</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">dense_MEAN_prior</span><span class="o">.</span><span class="n">size</span><span class="p">),</span>
    <span class="n">dense_MEAN_prior</span><span class="p">,</span>
    <span class="n">yerr</span><span class="o">=</span><span class="n">numpy</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">dense_SIGMA_prior</span><span class="p">),</span>
    <span class="n">fmt</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">,</span>
    <span class="n">capsize</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Analytical&quot;</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">errorbar</span><span class="p">(</span>
    <span class="n">numpy</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">dense_MEAN</span><span class="o">.</span><span class="n">size</span><span class="p">),</span>
    <span class="n">dense_MEAN</span><span class="p">,</span>
    <span class="n">yerr</span><span class="o">=</span><span class="n">numpy</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">dense_SIGMA</span><span class="p">),</span>
    <span class="n">fmt</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">,</span>
    <span class="n">capsize</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Dense without prior (previous)&quot;</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">2.25</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Parameter estimate&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Parameter index&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Parameter estimates</span><span class="se">\n</span><span class="s2">means: markers, sigma: lines&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span>
    <span class="s2">&quot;BE AWARE, I print only up to 2 digits. &quot;</span>
    <span class="s2">&quot;The sampling covariance will appear less &quot;</span>
    <span class="s2">&quot;accurate when printing more digits.&quot;</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Analytical covariance matrix:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dense_COV_prior</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sampling covariance matrix:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sampling_dense_COV_prior</span><span class="p">)</span>

<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Difference:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sampling_dense_COV_prior</span> <span class="o">-</span> <span class="n">dense_COV_prior</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/examples_1_-_Gaussian_inverse_problems_-_dense_forward_operator_58_0.png" src="../_images/examples_1_-_Gaussian_inverse_problems_-_dense_forward_operator_58_0.png" />
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
BE AWARE, I print only up to 2 digits. The sampling covariance will appear less accurate when printing more digits.

Analytical covariance matrix:
[[ 0.01  0.   -0.   -0.01 -0.   -0.  ]
 [ 0.    0.01  0.   -0.   -0.    0.  ]
 [-0.    0.    0.01 -0.   -0.   -0.  ]
 [-0.01 -0.   -0.    0.02 -0.    0.  ]
 [-0.   -0.   -0.   -0.    0.01 -0.  ]
 [-0.    0.   -0.    0.   -0.    0.01]]

Sampling covariance matrix:
[[ 0.01  0.   -0.   -0.01 -0.   -0.  ]
 [ 0.    0.01  0.   -0.   -0.    0.  ]
 [-0.    0.    0.01 -0.   -0.   -0.  ]
 [-0.01 -0.   -0.    0.02 -0.    0.  ]
 [-0.   -0.   -0.   -0.    0.01 -0.  ]
 [-0.    0.   -0.    0.   -0.    0.01]]

Difference:
[[-0.  0. -0.  0.  0.  0.]
 [ 0.  0. -0. -0. -0.  0.]
 [-0. -0.  0.  0.  0. -0.]
 [ 0. -0.  0.  0.  0. -0.]
 [ 0. -0.  0.  0.  0. -0.]
 [ 0.  0. -0. -0. -0. -0.]]
</pre></div></div>
</div>
<p>Again, our sampling corresponds to our analytical solution! The prior has two main impacts on the posterior:</p>
<ol class="arabic simple">
<li><p>The prior reduces variation (uncertainty) in the posterior by decreasing variances for parameters as well as tuning the correlation matrix to that of the prior. The amount the posterior correlation correspond to the prior correlation (which in this case was the identity matrix, no correlation) depends on the prior variance. Try changing it and visualizing the posterior correlation matrix (<strong>If you do this, also remember to re-tune the algorithm!</strong>).</p></li>
<li><p>The prior ‘pulls’ posterior means towards prior means. But, because there are correlations introduced through the data covariance matrix and the forward model matrix, a pull in one dimension might trade-off with a ‘push’ for another parameter. In more mathematical terms; in e.g. 2d, a Gaussian unit prior prefers the point (0.5, 0.5) to (1.0, 0.0).</p></li>
</ol>
</div>
<div class="section" id="Case-4---A-non-Gaussian-prior">
<h2>Case 4 - A non-Gaussian prior<a class="headerlink" href="#Case-4---A-non-Gaussian-prior" title="Permalink to this headline">¶</a></h2>
<p>In this final example we’ll test the influence of using a non-Gaussian prior. In this case, we can not solve the inverse problem analytically anymore. Our comparison can now only be done to a different posterior (i.e. the posterior from the previous case) to assess the impact of the prior.</p>
<p>We’ll use a prior that promotes sparsity. What this means, is that this prior ‘prefers’ solutions where many entries are zero (or many entries are at its mean, we can define what value that is). This is quantified by the Laplace distribution. In 1 dimensions it’s probability density function is proportional to the exponent of the L1 distance to a mean scaled by a disperion <span class="math notranslate nohighlight">\(b\)</span>:</p>
<div class="math notranslate nohighlight">
\[\text{Laplace}(x;\: \mu, b) \propto \exp \left( - \frac{|x-\mu|}{b}  \right)\]</div>
<p>By supplementing <code class="docutils literal notranslate"><span class="pre">hmc_tomography.Distributions.Laplace</span></code> with two vectors assumes <code class="docutils literal notranslate"><span class="pre">n</span></code> decoupled Laplace distributions. Next, we use Bayes’ rule (in <code class="docutils literal notranslate"><span class="pre">hmc_tomography.Distributions.BayesRule</span></code>) to combine the prior and likelihood into the posterior.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[28]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Create prior</span>
<span class="n">means</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">dimensions</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">dispersion</span> <span class="o">=</span> <span class="mf">0.025</span> <span class="o">*</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">dimensions</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">prior_laplace</span> <span class="o">=</span> <span class="n">hmc_tomography</span><span class="o">.</span><span class="n">Distributions</span><span class="o">.</span><span class="n">Laplace</span><span class="p">(</span><span class="n">means</span><span class="p">,</span> <span class="n">dispersion</span><span class="p">)</span>

<span class="n">posterior_laplace</span> <span class="o">=</span> <span class="n">hmc_tomography</span><span class="o">.</span><span class="n">Distributions</span><span class="o">.</span><span class="n">BayesRule</span><span class="p">([</span><span class="n">prior_laplace</span><span class="p">,</span> <span class="n">likelihood</span><span class="p">])</span>
</pre></div>
</div>
</div>
<p>And we go through all the motions again:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[29]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">hmc_tomography</span><span class="o">.</span><span class="n">Samplers</span><span class="o">.</span><span class="n">HMC</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span>
    <span class="n">posterior_laplace</span><span class="p">,</span>
    <span class="s2">&quot;bin_samples/tutorial_1_dense_covariance_with_laplace_prior.h5&quot;</span><span class="p">,</span>
    <span class="n">mass_matrix</span><span class="o">=</span><span class="n">hmc_tomography</span><span class="o">.</span><span class="n">MassMatrices</span><span class="o">.</span><span class="n">Unit</span><span class="p">(</span><span class="n">posterior_laplace</span><span class="o">.</span><span class="n">dimensions</span><span class="p">),</span>
    <span class="n">proposals</span><span class="o">=</span><span class="mi">80000</span><span class="p">,</span>
    <span class="n">initial_model</span><span class="o">=</span><span class="n">m_true</span><span class="p">,</span>
    <span class="n">time_step</span><span class="o">=</span><span class="mf">0.015</span><span class="p">,</span>
    <span class="n">online_thinning</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">overwrite_samples</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
sys:1: Warning:
Silently overwriting samples file (bin_samples/tutorial_1_dense_covariance_with_laplace_prior.h5) if it exists.
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "e855ff5e9e6e49b98b8479551ddd6d16", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>

</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[30]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="n">hmc_tomography</span><span class="o">.</span><span class="n">Post</span><span class="o">.</span><span class="n">Samples</span><span class="p">(</span>
    <span class="s2">&quot;bin_samples/tutorial_1_dense_covariance_with_laplace_prior.h5&quot;</span><span class="p">,</span> <span class="n">burn_in</span><span class="o">=</span><span class="mi">1000</span>
<span class="p">)</span> <span class="k">as</span> <span class="n">samples_posterior</span><span class="p">:</span>

    <span class="n">hmc_tomography</span><span class="o">.</span><span class="n">Post</span><span class="o">.</span><span class="n">Visualization</span><span class="o">.</span><span class="n">marginal_grid</span><span class="p">(</span>
        <span class="n">samples_posterior</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,],</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">12</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="n">sampling_dense_MEAN_prior_laplace</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">samples_posterior</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">sampling_dense_COV_prior_laplace</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">samples_posterior</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:])</span>

    <span class="n">sampling_dense_SIGMA_prior_laplace</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span>
        <span class="n">numpy</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">sampling_dense_COV_prior_laplace</span><span class="p">)</span> <span class="o">**</span> <span class="mf">0.5</span>
    <span class="p">)</span>

    <span class="n">sampling_dense_COR_prior_laplace</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">numpy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">sampling_dense_SIGMA_prior_laplace</span><span class="p">)</span>
        <span class="o">@</span> <span class="n">sampling_dense_COV_prior_laplace</span>
        <span class="o">@</span> <span class="n">numpy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">sampling_dense_SIGMA_prior_laplace</span><span class="p">)</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/examples_1_-_Gaussian_inverse_problems_-_dense_forward_operator_65_0.png" src="../_images/examples_1_-_Gaussian_inverse_problems_-_dense_forward_operator_65_0.png" />
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[31]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
    <span class="n">numpy</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">m_true</span><span class="o">.</span><span class="n">size</span><span class="p">),</span> <span class="n">m_true</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;True value&quot;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;v&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">errorbar</span><span class="p">(</span>
    <span class="n">numpy</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">sampling_dense_MEAN_prior_laplace</span><span class="o">.</span><span class="n">size</span><span class="p">),</span>
    <span class="n">sampling_dense_MEAN_prior_laplace</span><span class="p">,</span>
    <span class="n">yerr</span><span class="o">=</span><span class="n">numpy</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">sampling_dense_SIGMA_prior_laplace</span><span class="p">),</span>
    <span class="n">fmt</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">,</span>
    <span class="n">capsize</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Laplace prior&quot;</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">errorbar</span><span class="p">(</span>
    <span class="n">numpy</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">sampling_dense_MEAN_prior</span><span class="o">.</span><span class="n">size</span><span class="p">),</span>
    <span class="n">sampling_dense_MEAN_prior</span><span class="p">,</span>
    <span class="n">yerr</span><span class="o">=</span><span class="n">numpy</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">sampling_dense_SIGMA_prior</span><span class="p">),</span>
    <span class="n">fmt</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span>
    <span class="n">capsize</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Gaussian prior (previous)&quot;</span><span class="p">,</span>
<span class="p">)</span>


<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">2.25</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Parameter estimate&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Parameter index&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Parameter estimates</span><span class="se">\n</span><span class="s2">means: markers, sigma: lines&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/examples_1_-_Gaussian_inverse_problems_-_dense_forward_operator_66_0.png" src="../_images/examples_1_-_Gaussian_inverse_problems_-_dense_forward_operator_66_0.png" />
</div>
</div>
<p>It seems that using the Laplace prior gives us strong certainty for model parameters that are 0. Because of this, we can resolve trade-offs to other parameters better and the means of all parameters end up closer to the true solutions. This should be a logical consequence, because we added quite strong prior knowledge (our true solution is sparse) which helped to bring the posterior closer to the true solution.</p>
<p>We can see that for most parameters; uncertainties are lower by adding this effective prior:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[32]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="nb">print</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">sampling_dense_SIGMA_prior_laplace</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">sampling_dense_SIGMA_prior</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[0.13 0.17 0.03 0.22 0.06 0.03]
[0.11 0.11 0.1  0.12 0.11 0.11]
</pre></div></div>
</div>
<p>Looking at the marginals of the two posteriors also well illustrates the non-Gaussianity, as well as the increased accuracy of the posterior.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[33]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="n">hmc_tomography</span><span class="o">.</span><span class="n">Post</span><span class="o">.</span><span class="n">Samples</span><span class="p">(</span>
    <span class="s2">&quot;bin_samples/tutorial_1_dense_covariance_with_laplace_prior.h5&quot;</span><span class="p">,</span> <span class="n">burn_in</span><span class="o">=</span><span class="mi">1000</span>
<span class="p">)</span> <span class="k">as</span> <span class="n">samples_posterior_laplace</span><span class="p">,</span> <span class="n">hmc_tomography</span><span class="o">.</span><span class="n">Post</span><span class="o">.</span><span class="n">Samples</span><span class="p">(</span>
    <span class="s2">&quot;bin_samples/tutorial_1_dense_covariance_with_gaussian_prior.h5&quot;</span><span class="p">,</span> <span class="n">burn_in</span><span class="o">=</span><span class="mi">1000</span>
<span class="p">)</span> <span class="k">as</span> <span class="n">samples_posterior_gaussian</span><span class="p">,</span> <span class="n">hmc_tomography</span><span class="o">.</span><span class="n">Post</span><span class="o">.</span><span class="n">Samples</span><span class="p">(</span>
    <span class="s2">&quot;bin_samples/tutorial_1_dense_covariance.h5&quot;</span><span class="p">,</span> <span class="n">burn_in</span><span class="o">=</span><span class="mi">1000</span>
<span class="p">)</span> <span class="k">as</span> <span class="n">samples_posterior</span><span class="p">:</span>

    <span class="n">fix</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>

    <span class="n">ylims</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>

        <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span>
            <span class="n">samples_posterior_gaussian</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:],</span>
            <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
            <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Gaussian&quot;</span><span class="p">,</span>
            <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
            <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span>
            <span class="n">samples_posterior_laplace</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:],</span>
            <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
            <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Laplace&quot;</span><span class="p">,</span>
            <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
            <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">])</span>
        <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Parameter </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Probability density&quot;</span><span class="p">)</span>

        <span class="n">ylims</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">get_ylim</span><span class="p">())</span>

    <span class="n">ymax</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">ylims</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>

        <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">m_true</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">m_true</span><span class="p">[</span><span class="n">i</span><span class="p">]],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">ymax</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;true value&quot;</span><span class="p">)</span>
        <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="n">ymax</span><span class="p">])</span>

    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/examples_1_-_Gaussian_inverse_problems_-_dense_forward_operator_70_0.png" src="../_images/examples_1_-_Gaussian_inverse_problems_-_dense_forward_operator_70_0.png" />
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="2%20-%20Gaussian%20inverse%20problems%20-%20sparse%20forward%20operator.html" class="btn btn-neutral float-right" title="Tutorial 2 - Gaussian inverse problems - sparse forward operator" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="0.2%20-%20Tuning%20Hamiltonian%20Monte%20Carlo.html" class="btn btn-neutral float-left" title="Tutorial 0.2 - Tuning Hamiltonian Monte Carlo" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2019-2020, Andrea Zunino, Andreas Fichtner, Lars Gebraad

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>